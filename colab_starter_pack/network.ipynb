{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# WikiNet\n",
        "\n",
        "This is the python notebook file to train and run experiments from https://medium.com/@alexanderjhurtado/3f149676fbf3\n",
        "\n",
        "The github with more files and information can be found at: https://github.com/alexanderjhurtado/cs224w_final\n",
        "\n",
        "WikiNet is a model we have created to tackle the target prediction problem on the Wikispeedia dataset. Namely, given a sequence of articles clicked by a plae, our task is to predict the final target article the user is searching for. The following code is of our model definition, training, and evaluation for our experiments."
      ],
      "metadata": {
        "id": "kEi9i7wEkjIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we begin by installing the necessary libraries!"
      ],
      "metadata": {
        "id": "lXXXMdUGp0hR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fq2m29soI-Y"
      },
      "outputs": [],
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we import all libraries that will be used by our code."
      ],
      "metadata": {
        "id": "ARdVxxwbp43X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zr95n5Vzoqq3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import time\n",
        "import networkx as nx\n",
        "from torch_geometric.utils import from_networkx\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCN, GAT, GraphSAGE\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download *graph_with_features.gml* and *paths_and_labels.tsv* from the GitHub repository. It can be found under the *colab_starter_pack/* folder. Upload these files to the current runtime to be able to run the following cell, which gets our fully preprocessed graph and path dataset. To see our data preprocessing code, look under the *extra_data_processing/* folder in our repository. "
      ],
      "metadata": {
        "id": "G-OTMn78p-yD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJK41jk8osx9"
      },
      "outputs": [],
      "source": [
        "nx_graph = nx.read_gml('graph_with_features.gml')\n",
        "G = from_networkx(nx_graph, group_node_attrs=['out_degree', 'in_degree', 'category_multi_hot', 'article_embed'])\n",
        "\n",
        "path_data = pd.read_csv('paths_and_labels.tsv', sep='\\t', header=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function will be called during training and evaluation to evaluate the model on the validation and test datasets."
      ],
      "metadata": {
        "id": "mGigWEo0rH2M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xL8mhtOrovBA"
      },
      "outputs": [],
      "source": [
        "def get_evaluation_metrics(model, device, dataloader, dataset_size):\n",
        "    model.eval()\n",
        "    avg_loss = 0\n",
        "    num_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(dataloader):\n",
        "            # get data\n",
        "            inputs, labels = data['indices'].to(device), data['label'].to(device)\n",
        "            outputs = model(inputs)\n",
        "            # get loss\n",
        "            loss = F.nll_loss(outputs, labels)\n",
        "            avg_loss += loss.item()\n",
        "            # get accuracy\n",
        "            pred = outputs.argmax(dim=1)\n",
        "            correct = (pred == labels).sum()\n",
        "            num_correct += correct\n",
        "    acc = int(num_correct) / dataset_size\n",
        "    avg_loss /= dataset_size\n",
        "    return acc, avg_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This defines the dataset class we use to represent our path data."
      ],
      "metadata": {
        "id": "2ZGu5CTXtQPO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tCnsYEvowvO"
      },
      "outputs": [],
      "source": [
        "class CustomPathDataset(Dataset):\n",
        "    def __init__(self, path_data):\n",
        "        self.x = path_data[0].apply(json.loads)\n",
        "        self.labels = path_data[1]\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.LongTensor(self.x[idx])\n",
        "        label = self.labels[idx]\n",
        "        sample = {\"indices\": x, \"label\": label}\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the class definition for our baseline model, an LSTM. Run this cell to be able to train the baseline model."
      ],
      "metadata": {
        "id": "mLISGR7dtTkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Baseline(torch.nn.Module):\n",
        "    def __init__(self, graph, device, node_embed_size=64, lstm_hidden_size=32):\n",
        "        super().__init__()\n",
        "        self.graphX = graph.x.to(device)\n",
        "        self.graphEdgeIndex = graph.edge_index.to(device)\n",
        "        self.lstm_input_size = node_embed_size\n",
        "        self.lstm = nn.LSTM(input_size=self.lstm_input_size,\n",
        "                            hidden_size=lstm_hidden_size,\n",
        "                            batch_first=True)\n",
        "        self.pred_head = nn.Linear(lstm_hidden_size, self.graphX.shape[0])\n",
        "\n",
        "    def forward(self, indices):\n",
        "        node_emb = self.graphX\n",
        "        node_emb_with_padding = torch.cat([node_emb, torch.zeros((1, self.lstm_input_size)).to(device)])\n",
        "        paths = node_emb_with_padding[indices]\n",
        "        _, (h_n, _) = self.lstm(paths)\n",
        "        predictions = self.pred_head(torch.squeeze(h_n))\n",
        "        return F.log_softmax(predictions, dim=1)"
      ],
      "metadata": {
        "id": "MSMEyv6Ytcid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the class definition for our Graph Neural Network - based model. In our experiments, we ran three different GNNs: Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), and GraphSAGE. Our GraphSAGE model performed the best and is the current model used in our class definition. If you would like to use GCN or GAT, simply replace `self.gnn = GraphSAGE(...)` with `self.gnn = GCN(...)` or `self.gnn = GAT(...)`, respectively. The arguments are the same for all 3 models.\n",
        "\n",
        "This cell also defines our model weights file. This file will be generated during training, storing the weights for the best model based on validation accuracy during training."
      ],
      "metadata": {
        "id": "ysVSRJp_uGhR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6nU8MS_oyC7"
      },
      "outputs": [],
      "source": [
        "MODEL_WEIGHT_PATH = \"model_weights.pth\"\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, graph, device, sequence_path_length=32, gnn_hidden_size=128, node_embed_size=64, lstm_hidden_size=32):\n",
        "        super().__init__()\n",
        "        self.graphX = graph.x.to(device)\n",
        "        self.graphEdgeIndex = graph.edge_index.to(device)\n",
        "        self.gnn = GraphSAGE(in_channels=self.graphX.shape[1], \n",
        "                       hidden_channels=gnn_hidden_size, \n",
        "                       num_layers=3, \n",
        "                       out_channels=node_embed_size, \n",
        "                       dropout=0.1)\n",
        "        self.batch_norm_lstm = nn.BatchNorm1d(sequence_path_length)\n",
        "        self.batch_norm_linear = nn.BatchNorm1d(lstm_hidden_size)\n",
        "        self.lstm_input_size = node_embed_size\n",
        "        self.lstm = nn.LSTM(input_size=self.lstm_input_size,\n",
        "                            hidden_size=lstm_hidden_size,\n",
        "                            batch_first=True)\n",
        "        self.pred_head = nn.Linear(lstm_hidden_size, self.graphX.shape[0])\n",
        "\n",
        "    def forward(self, indices):\n",
        "        node_emb = self.gnn(self.graphX, self.graphEdgeIndex)\n",
        "        node_emb_with_padding = torch.cat([node_emb, torch.zeros((1, self.lstm_input_size)).to(device)])\n",
        "        paths = node_emb_with_padding[indices]\n",
        "        paths = self.batch_norm_lstm(paths)\n",
        "        _, (h_n, _) = self.lstm(paths)\n",
        "        h_n = self.batch_norm_linear(torch.squeeze(h_n))\n",
        "        predictions = self.pred_head(h_n)\n",
        "        return F.log_softmax(predictions, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we set up our `train / val / test` split as `90 / 5 / 5`. Moreover, we define our hyperparameters, including the learning rate, our optimizer (Adam), and the batch size.\n",
        "\n",
        "To train on the baseline model instead of the GNN-based model, replace the line `model = Model(G, device).to(device)` to `model = Baseline(G, device).to(device)`."
      ],
      "metadata": {
        "id": "8XwpRS2VwnV4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR5HTLUgo0lX"
      },
      "outputs": [],
      "source": [
        "# set up our model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Model(G, device).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# get our dataset + splits\n",
        "dataset = CustomPathDataset(path_data)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "test_size = int(0.05 * len(dataset))\n",
        "val_size = len(dataset) - train_size - test_size\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# set up for training + validation\n",
        "batch_size = 1024\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "validloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is our training script. We train our model for 200 epochs and print training loss, validation loss, validation accuracy, and time spent for each epoch.\n",
        "\n",
        "Moreover, we train by running one batch through the model at a time and using the Negative Log Likelihood loss function. We also save the model weights for the best validation accuracy we see after an epoch. These weights will be used in the evaluation step."
      ],
      "metadata": {
        "id": "X4qpQl1YxGUK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mxsa2R8hp9_Q"
      },
      "outputs": [],
      "source": [
        "best_acc = 0\n",
        "training_losses = []\n",
        "validation_losses = []\n",
        "validation_accs = []\n",
        "model.train()\n",
        "for epoch in range(200):  # loop over the dataset multiple times\n",
        "    print('Epoch:', epoch+1)\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    start_time = time.time()\n",
        "    for i, data in enumerate(trainloader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data['indices'].to(device), data['label'].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = F.nll_loss(outputs, labels)\n",
        "        epoch_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # validate epoch and print results\n",
        "    training_losses.append(epoch_loss / train_size)\n",
        "    print('Training Loss:', training_losses[-1])\n",
        "    acc, valid_loss = get_evaluation_metrics(model, device, validloader, val_size)\n",
        "    validation_losses.append(valid_loss)\n",
        "    validation_accs.append(acc)\n",
        "    if acc > best_acc:\n",
        "        torch.save(model.state_dict(), MODEL_WEIGHT_PATH)\n",
        "        best_acc = acc\n",
        "    print(\"Validation accuracy:\", acc)\n",
        "    print(\"Validation loss:\", valid_loss)\n",
        "    print('Time elapsed:', time.time() - start_time)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code runs evaluation on our test dataset. In particular, it uses the weights from the best validation accuracy to obtain our test accuracy. If you do not want to use those weights, you can comment out the line `model.load_state_dict(...)` to use the final weights from training instead.\n",
        "\n",
        "This cell will print out the \"loss\" and accuracy on the testing dataset."
      ],
      "metadata": {
        "id": "Xig9d5Kxxilv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_sObjK9o5_4"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(MODEL_WEIGHT_PATH))\n",
        "model.eval()\n",
        "acc, test_loss = get_evaluation_metrics(model, device, testloader, test_size)\n",
        "print(\"Test accuracy:\", acc)\n",
        "print(\"Test loss:\", test_loss)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}