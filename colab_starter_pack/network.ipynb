{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fq2m29soI-Y"
      },
      "outputs": [],
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zr95n5Vzoqq3"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import time\n",
        "import networkx as nx\n",
        "from torch_geometric.utils import from_networkx\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCN, GAT, GraphSAGE\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJK41jk8osx9"
      },
      "outputs": [],
      "source": [
        "nx_graph = nx.read_gml('graph_with_features.gml')\n",
        "G = from_networkx(nx_graph, group_node_attrs=['out_degree', 'in_degree', 'category_multi_hot', 'article_embed'])\n",
        "\n",
        "path_data = pd.read_csv('data_by_index.tsv', sep='\\t', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xL8mhtOrovBA"
      },
      "outputs": [],
      "source": [
        "def get_evaluation_metrics(model, device, dataloader, dataset_size):\n",
        "    model.eval()\n",
        "    avg_loss = 0\n",
        "    num_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(dataloader):\n",
        "            # get data\n",
        "            inputs, labels = data['indices'].to(device), data['label'].to(device)\n",
        "            outputs = model(inputs)\n",
        "            # get loss\n",
        "            loss = F.nll_loss(outputs, labels)\n",
        "            avg_loss += loss.item()\n",
        "            # get accuracy\n",
        "            pred = outputs.argmax(dim=1)\n",
        "            correct = (pred == labels).sum()\n",
        "            num_correct += correct\n",
        "    acc = int(num_correct) / dataset_size\n",
        "    avg_loss /= dataset_size\n",
        "    return acc, avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tCnsYEvowvO"
      },
      "outputs": [],
      "source": [
        "class CustomPathDataset(Dataset):\n",
        "    def __init__(self, path_data):\n",
        "        self.x = path_data[0].apply(json.loads)\n",
        "        self.labels = path_data[1]\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.LongTensor(self.x[idx])\n",
        "        label = self.labels[idx]\n",
        "        sample = {\"indices\": x, \"label\": label}\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6nU8MS_oyC7"
      },
      "outputs": [],
      "source": [
        "MODEL_WEIGHT_PATH = \"graphsage_weights.pth\"\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, graph, device, gnn_hidden_size=128, node_embed_size=64, lstm_hidden_size=32):\n",
        "        super().__init__()\n",
        "        self.graphX = graph.x.to(device)\n",
        "        self.graphEdgeIndex = graph.edge_index.to(device)\n",
        "        self.gnn = GraphSAGE(in_channels=self.graphX.shape[1], \n",
        "                       hidden_channels=gnn_hidden_size, \n",
        "                       num_layers=3, \n",
        "                       out_channels=node_embed_size, \n",
        "                       dropout=0.1)\n",
        "        self.batch_norm_lstm = nn.BatchNorm1d(32)\n",
        "        self.batch_norm_linear = nn.BatchNorm1d(32)\n",
        "        self.lstm_input_size = node_embed_size\n",
        "        self.lstm = nn.LSTM(input_size=self.lstm_input_size,\n",
        "                            hidden_size=lstm_hidden_size,\n",
        "                            batch_first=True)\n",
        "        self.pred_head = nn.Linear(lstm_hidden_size, self.graphX.shape[0])\n",
        "\n",
        "    def forward(self, indices):\n",
        "        node_emb = self.gnn(self.graphX, self.graphEdgeIndex)\n",
        "        node_emb_with_padding = torch.cat([node_emb, torch.zeros((1, self.lstm_input_size)).to(device)])\n",
        "        paths = node_emb_with_padding[indices]\n",
        "        paths = self.batch_norm_lstm(paths)\n",
        "        _, (h_n, _) = self.lstm(paths)\n",
        "        h_n = self.batch_norm_linear(torch.squeeze(h_n))\n",
        "        predictions = self.pred_head(h_n)\n",
        "        return F.log_softmax(predictions, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR5HTLUgo0lX"
      },
      "outputs": [],
      "source": [
        "# set up our model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Model(G, device).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # NOTE: lr=0.01\n",
        "\n",
        "# get our dataset + splits\n",
        "dataset = CustomPathDataset(path_data)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "test_size = int(0.05 * len(dataset))\n",
        "val_size = len(dataset) - train_size - test_size\n",
        "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# set up for training + validation\n",
        "batch_size = 1024\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "validloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mxsa2R8hp9_Q"
      },
      "outputs": [],
      "source": [
        "best_acc = 0\n",
        "training_losses = []\n",
        "validation_losses = []\n",
        "validation_accs = []\n",
        "model.train()\n",
        "for epoch in range(200):  # loop over the dataset multiple times\n",
        "    print('Epoch:', epoch+1)\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    start_time = time.time()\n",
        "    for i, data in enumerate(trainloader):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data['indices'].to(device), data['label'].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = F.nll_loss(outputs, labels)\n",
        "        epoch_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # validate epoch and print results\n",
        "    training_losses.append(epoch_loss / train_size)\n",
        "    print('Training Loss:', training_losses[-1])\n",
        "    acc, valid_loss = get_evaluation_metrics(model, device, validloader, val_size)\n",
        "    validation_losses.append(valid_loss)\n",
        "    validation_accs.append(acc)\n",
        "    if acc > best_acc:\n",
        "        torch.save(model.state_dict(), MODEL_WEIGHT_PATH)\n",
        "        best_acc = acc\n",
        "    print(\"Validation accuracy:\", acc)\n",
        "    print(\"Validation loss:\", valid_loss)\n",
        "    print('Time elapsed:', time.time() - start_time)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_sObjK9o5_4"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(MODEL_WEIGHT_PATH))\n",
        "model.eval()\n",
        "acc, test_loss = get_evaluation_metrics(model, device, testloader, test_size)\n",
        "print(\"Test accuracy:\", acc)\n",
        "print(\"Test loss:\", test_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOO_CkGApp2Z"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}